{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8449025,"sourceType":"datasetVersion","datasetId":5034843}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers pandas sklearn openpyxl\n!pip install translate\n!pip install indic-transliteration","metadata":{"id":"XxzQmfB7IxXN","execution":{"iopub.status.busy":"2024-05-18T10:15:26.820944Z","iopub.execute_input":"2024-05-18T10:15:26.821233Z","iopub.status.idle":"2024-05-18T10:15:56.192276Z","shell.execute_reply.started":"2024-05-18T10:15:26.821205Z","shell.execute_reply":"2024-05-18T10:15:56.191245Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.1.4)\nCollecting sklearn\n  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m More information is available at\n  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\n\u001b[?25hCollecting translate\n  Downloading translate-3.6.1-py2.py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from translate) (8.1.7)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from translate) (5.2.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from translate) (2.31.0)\nCollecting libretranslatepy==2.1.1 (from translate)\n  Downloading libretranslatepy-2.1.1-py3-none-any.whl.metadata (233 bytes)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->translate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->translate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->translate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->translate) (2024.2.2)\nDownloading translate-3.6.1-py2.py3-none-any.whl (12 kB)\nDownloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)\nInstalling collected packages: libretranslatepy, translate\nSuccessfully installed libretranslatepy-2.1.1 translate-3.6.1\nCollecting indic-transliteration\n  Downloading indic_transliteration-2.3.59-py3-none-any.whl.metadata (1.4 kB)\nCollecting backports.functools-lru-cache (from indic-transliteration)\n  Downloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from indic-transliteration) (2023.12.25)\nRequirement already satisfied: typer in /opt/conda/lib/python3.10/site-packages (from indic-transliteration) (0.9.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from indic-transliteration) (0.10.2)\nCollecting roman (from indic-transliteration)\n  Downloading roman-4.2-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer->indic-transliteration) (8.1.7)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from typer->indic-transliteration) (4.9.0)\nDownloading indic_transliteration-2.3.59-py3-none-any.whl (152 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.1/152.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl (6.7 kB)\nDownloading roman-4.2-py3-none-any.whl (5.5 kB)\nInstalling collected packages: roman, backports.functools-lru-cache, indic-transliteration\nSuccessfully installed backports.functools-lru-cache-2.0.0 indic-transliteration-2.3.59 roman-4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom indic_transliteration import sanscript\nfrom indic_transliteration.sanscript import transliterate\nfrom translate import Translator\n\n# Load the model and tokenizer\nmodel_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\n# Function to get sentence embeddings\ndef get_embeddings(text_list):\n    tokens = tokenizer(text_list, padding=True, truncation=True, return_tensors=\"pt\")\n    with torch.no_grad():\n        embeddings = model(**tokens).last_hidden_state.mean(dim=1)\n    return embeddings\n\n# Read the dataset from the provided path\nfile_path = '/kaggle/input/merge-database-file/Merge Database File.xlsx'\ndf = pd.read_excel(file_path)\n\n# Ensure the dataset contains a column named 'bhajan'\nif 'bhajan' not in df.columns:\n    raise ValueError(\"The dataset must contain a column named 'bhajan'.\")\n\n# Clean the 'bhajan' column to ensure all entries are strings\ndf['bhajan'] = df['bhajan'].astype(str).fillna('')\n\n# Remove any empty strings to avoid issues with the tokenizer\ndf = df[df['bhajan'].str.strip() != '']\n\n# Create a new column with Romanized (transliterated) bhajans\ndf['romanized_bhajan'] = df['bhajan'].apply(lambda x: transliterate(x, sanscript.DEVANAGARI, sanscript.ITRANS))\n\n# Get embeddings for original and Romanized bhajans\noriginal_embeddings = get_embeddings(df['bhajan'].tolist())\nromanized_embeddings = get_embeddings(df['romanized_bhajan'].tolist())\n\n# Combine both embeddings\ncombined_embeddings = torch.cat((original_embeddings, romanized_embeddings), dim=0)\n\n# Extend the DataFrame to match the combined embeddings\nextended_df = pd.concat([df, df], ignore_index=True)\n\n# Save the embeddings to a file\ntorch.save(original_embeddings, 'embeddings.pt')\nprint(\"Embeddings saved successfully.\")\n\n# Function to find the most similar bhajan\ndef find_most_similar(query):\n    # Function to transliterate English query to Devanagari\n    def transliterate_to_devanagari(query):\n        return transliterate(query, sanscript.ITRANS, sanscript.DEVANAGARI)\n\n    if all(ord(char) < 128 for char in query):  # Check if query is in English\n        try:\n            # Translate English query to Marathi\n            translator = Translator(to_lang='mr', from_lang='en')\n            translated_text = translator.translate(query)\n            query = translated_text\n        except Exception as e:\n            print(\"Error occurred during translation:\", e)\n            return None\n\n    query_embedding = get_embeddings([query])\n    similarities = cosine_similarity(query_embedding, original_embeddings)\n    most_similar_idx = similarities.argmax()\n    return extended_df.iloc[most_similar_idx]['bhajan']\n","metadata":{"id":"TS-DCoJxImuP","execution":{"iopub.status.busy":"2024-05-18T10:15:56.194916Z","iopub.execute_input":"2024-05-18T10:15:56.195360Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a881bc7d8a7349e196a4ec9eeb9e3b9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"121122b590a7461fa34ced49b01ea095"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f76e56ceee714025989b3990d37ddb7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d61d4beb860845b786784bde41256502"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b621331900743e49a664dcf623b608d"}},"metadata":{}}]},{"cell_type":"code","source":"# Prompt the user for a query\nquery = input(\"Enter your search query: \")\nmost_similar_bhajan = find_most_similar(query)\nif most_similar_bhajan:\n    print(\"Most similar bhajan:\", most_similar_bhajan)\nelse:\n    print(\"Failed to find the most similar bhajan.\")","metadata":{"id":"ZjN3enTNInzG","trusted":true},"execution_count":null,"outputs":[]}]}